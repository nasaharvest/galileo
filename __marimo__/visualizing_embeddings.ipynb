{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "name": "intro"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre class='text-xs'>&#x27;\\n# Making embeddings from real data\\n\\nThis notebook demonstrates how to make embeddings with the Galileo models using real data (exported by our GEE exporter).\\n\\nOur GEE exporter is called using the following script:\\n```python\\nfrom datetime import date\\n\\nfrom src.data import EarthEngineExporter\\nfrom src.data.earthengine import EEBoundingBox\\n\\n# to export points\\nEarthEngineExporter(dest_bucket=&quot;bucket_name&quot;).export_for_latlons(df)\\n# to export a bounding box\\nbbox = EEBoundingBox(min_lat=49.017835,min_lon-123.303680,max_lat=49.389519,max_lon-122.792816)\\nEarthEngineExporter(dest_bucket=&quot;bucket_name&quot;).export_for_bbox(bbox, start_date=date(2024, 1, 1), end_date=(2025, 1, 1))\\n```\\n&#x27;</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Making embeddings from real data\n",
    "\n",
    "This notebook demonstrates how to make embeddings with the Galileo models using real data (exported by our GEE exporter).\n",
    "\n",
    "Our GEE exporter is called using the following script:\n",
    "```python\n",
    "from datetime import date\n",
    "\n",
    "from src.data import EarthEngineExporter\n",
    "from src.data.earthengine import EEBoundingBox\n",
    "\n",
    "# to export points\n",
    "EarthEngineExporter(dest_bucket=\"bucket_name\").export_for_latlons(df)\n",
    "# to export a bounding box\n",
    "bbox = EEBoundingBox(min_lat=49.017835,min_lon-123.303680,max_lat=49.389519,max_lon-122.792816)\n",
    "EarthEngineExporter(dest_bucket=\"bucket_name\").export_for_bbox(bbox, start_date=date(2024, 1, 1), end_date=(2025, 1, 1))\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "name": "imports"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ STARTING: Importing libraries...\n",
      "âœ… SUCCESS: All libraries imported successfully!\n",
      "ğŸ“Š Using PyTorch version: 2.2.1\n",
      "ğŸ“Š Using NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ STARTING: Importing libraries...\")\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.config import DATA_FOLDER, NORMALIZATION_DICT_FILENAME\n",
    "from src.data.dataset import Dataset, Normalizer\n",
    "from src.galileo import Encoder\n",
    "from src.masking import MaskedOutput\n",
    "from src.utils import config_dir\n",
    "\n",
    "print(\"âœ… SUCCESS: All libraries imported successfully!\")\n",
    "print(f\"ğŸ“Š Using PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ“Š Using NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {
    "marimo": {
     "name": "load_data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ STEP 1: Loading normalization values...\n",
      "âœ… Loaded normalization dict with 6 entries\n",
      "ğŸ“ Config directory: /Users/rfievet3/projects/CSSE/galileo/config\n",
      "ğŸ“„ Normalization file: normalization.json\n",
      "ğŸ”„ Creating normalizer...\n",
      "âœ… Normalizer created successfully\n",
      "ğŸ”„ Loading TIF file...\n",
      "ğŸ“ TIF path: data/tifs/min_lat=-27.6721_min_lon=25.6796_max_lat=-27.663_max_lon=25.6897_dates=2022-01-01_2023-12-31.tif\n",
      "ğŸ“Š TIF exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TIF loaded and normalized successfully!\n",
      "ğŸ“Š Dataset output type: <class 'src.data.dataset.DatasetOutput'>\n",
      "ğŸ“Š Space-time data shape: (102, 114, 24, 13)\n",
      "ğŸ“Š Space data shape: (102, 114, 16)\n",
      "ğŸ“Š Time data shape: (24, 6)\n",
      "ğŸ“Š Static data shape: (18,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "First, we'll load a dataset output using one of the example training tifs in `data/tifs`. We also normalize it using the same normalization stats we used during training.\n",
    "\"\"\"\n",
    "print(\"ğŸ”„ STEP 1: Loading normalization values...\")\n",
    "normalizing_dict = Dataset.load_normalization_values(\n",
    "    path=config_dir / NORMALIZATION_DICT_FILENAME\n",
    ")\n",
    "print(f\"âœ… Loaded normalization dict with {len(normalizing_dict)} entries\")\n",
    "print(f\"ğŸ“ Config directory: {config_dir}\")\n",
    "print(f\"ğŸ“„ Normalization file: {NORMALIZATION_DICT_FILENAME}\")\n",
    "\n",
    "print(\"ğŸ”„ Creating normalizer...\")\n",
    "normalizer = Normalizer(std=True, normalizing_dicts=normalizing_dict)\n",
    "print(\"âœ… Normalizer created successfully\")\n",
    "\n",
    "print(\"ğŸ”„ Loading TIF file...\")\n",
    "tif_path = Path(\n",
    "    \"data/tifs/min_lat=-27.6721_min_lon=25.6796_max_lat=-27.663_max_lon=25.6897_dates=2022-01-01_2023-12-31.tif\"\n",
    ")\n",
    "print(f\"ğŸ“ TIF path: {tif_path}\")\n",
    "print(f\"ğŸ“Š TIF exists: {tif_path.exists()}\")\n",
    "\n",
    "dataset_output = Dataset._tif_to_array(tif_path).normalize(normalizer)\n",
    "print(\"âœ… TIF loaded and normalized successfully!\")\n",
    "print(f\"ğŸ“Š Dataset output type: {type(dataset_output)}\")\n",
    "print(f\"ğŸ“Š Space-time data shape: {dataset_output.space_time_x.shape}\")\n",
    "print(\n",
    "    f\"ğŸ“Š Space data shape: {dataset_output.space_x.shape if hasattr(dataset_output, 'space_x') else 'N/A'}\"\n",
    ")\n",
    "print(\n",
    "    f\"ğŸ“Š Time data shape: {dataset_output.time_x.shape if hasattr(dataset_output, 'time_x') else 'N/A'}\"\n",
    ")\n",
    "print(\n",
    "    f\"ğŸ“Š Static data shape: {dataset_output.static_x.shape if hasattr(dataset_output, 'static_x') else 'N/A'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "name": "visualize_data"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ STEP 2: Visualizing S2-RGB bands...\n",
      "ğŸ“Š Extracting RGB bands [4, 3, 2] from timestep 0\n",
      "ğŸ“Š RGB data shape: (102, 114, 3)\n",
      "ğŸ“Š RGB data range: [0.306, 0.826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RGB visualization complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This tif captures the Vaal river near the [Bloemhof dam](https://en.wikipedia.org/wiki/Bloemhof_Dam).\n",
    "We can visualize the S2-RGB bands from the first timestep:\n",
    "\"\"\"\n",
    "print(\"ğŸ”„ STEP 2: Visualizing S2-RGB bands...\")\n",
    "print(\"ğŸ“Š Extracting RGB bands [4, 3, 2] from timestep 0\")\n",
    "rgb_data = dataset_output.space_time_x[:, :, 0, [4, 3, 2]].astype(np.float32)\n",
    "print(f\"ğŸ“Š RGB data shape: {rgb_data.shape}\")\n",
    "print(f\"ğŸ“Š RGB data range: [{rgb_data.min():.3f}, {rgb_data.max():.3f}]\")\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(rgb_data)\n",
    "plt.title(\"S2-RGB bands from first timestep\")\n",
    "plt.show()\n",
    "print(\"âœ… RGB visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "name": "load_model"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ STEP 3: Loading Galileo nano model...\n",
      "ğŸ“ Model path: /Users/rfievet3/projects/CSSE/galileo/data/models/nano\n",
      "ğŸ“Š Model path exists: True\n",
      "âœ… Model loaded successfully!\n",
      "ğŸ“Š Model type: <class 'src.galileo.Encoder'>\n",
      "ğŸ“Š Model device: cpu\n",
      "ğŸ“Š Model parameters: 1,037,856\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We'll use the nano model (which is conveniently stored in git) to make these embeddings.\n",
    "\"\"\"\n",
    "print(\"ğŸ”„ STEP 3: Loading Galileo nano model...\")\n",
    "model_path = DATA_FOLDER / \"models/nano\"\n",
    "print(f\"ğŸ“ Model path: {model_path}\")\n",
    "print(f\"ğŸ“Š Model path exists: {model_path.exists()}\")\n",
    "\n",
    "model = Encoder.load_from_folder(model_path)\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"ğŸ“Š Model type: {type(model)}\")\n",
    "print(f\"ğŸ“Š Model device: {next(model.parameters()).device}\")\n",
    "print(f\"ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {
    "marimo": {
     "name": "define_embedding_function"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ STEP 4: Defining embedding function...\n",
      "âœ… Embedding function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "print(\"ğŸ”„ STEP 4: Defining embedding function...\")\n",
    "\n",
    "def make_embeddings(\n",
    "    model: Any,\n",
    "    datasetoutput: Any,\n",
    "    window_size: int,\n",
    "    patch_size: int,\n",
    "    batch_size: int = 128,\n",
    "    device: Any = None,\n",
    ") -> Any:\n",
    "    print(\"ğŸ”„ Starting embedding generation...\")\n",
    "    print(f\"ğŸ“Š Window size: {window_size}\")\n",
    "    print(f\"ğŸ“Š Patch size: {patch_size}\")\n",
    "    print(f\"ğŸ“Š Batch size: {batch_size}\")\n",
    "    print(f\"ğŸ“Š Device: {device}\")\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    print(\"âœ… Model set to evaluation mode\")\n",
    "\n",
    "    output_embeddings_list = []\n",
    "    batch_count = 0\n",
    "\n",
    "    for i in tqdm(\n",
    "        datasetoutput.in_pixel_batches(batch_size=batch_size, window_size=window_size)\n",
    "    ):\n",
    "        batch_count += 1\n",
    "        if batch_count % 10 == 0:\n",
    "            print(f\"ğŸ”„ Processing batch {batch_count}...\")\n",
    "\n",
    "        masked_output = MaskedOutput.from_datasetoutput(i, device=device)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(\n",
    "                masked_output.space_time_x.float(),\n",
    "                masked_output.space_x.float(),\n",
    "                masked_output.time_x.float(),\n",
    "                masked_output.static_x.float(),\n",
    "                masked_output.space_time_mask,\n",
    "                masked_output.space_mask,\n",
    "                # lets mask inputs which will be the same for\n",
    "                # all pixels in the DatasetOutput\n",
    "                torch.ones_like(masked_output.time_mask),\n",
    "                torch.ones_like(masked_output.static_mask),\n",
    "                masked_output.months.long(),\n",
    "                patch_size=patch_size,\n",
    "            )\n",
    "\n",
    "            embeddings = model.average_tokens(*model_output[:-1]).cpu().numpy()\n",
    "            output_embeddings_list.append(embeddings)\n",
    "\n",
    "    print(f\"âœ… Processed {batch_count} batches total\")\n",
    "    print(\"ğŸ”„ Concatenating embeddings...\")\n",
    "\n",
    "    output_embeddings = np.concatenate(output_embeddings_list, axis=0)\n",
    "    print(f\"ğŸ“Š Concatenated embeddings shape: {output_embeddings.shape}\")\n",
    "\n",
    "    # reshape the embeddings to H, W, D\n",
    "    # first - how many \"height batches\" and \"width batches\" did we get?\n",
    "    h_b = datasetoutput.space_time_x.shape[0] // window_size\n",
    "    w_b = datasetoutput.space_time_x.shape[1] // window_size\n",
    "\n",
    "    print(f\"ğŸ“Š Reshaping: height_batches={h_b}, width_batches={w_b}\")\n",
    "\n",
    "    reshaped_embeddings = rearrange(\n",
    "        output_embeddings, \"(h_b w_b) d -> h_b w_b d\", h_b=h_b, w_b=w_b\n",
    "    )\n",
    "    print(f\"ğŸ“Š Final embeddings shape: {reshaped_embeddings.shape}\")\n",
    "\n",
    "    return reshaped_embeddings\n",
    "\n",
    "print(\"âœ… Embedding function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {
    "marimo": {
     "name": "generate_embeddings"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ STEP 5: Generating embeddings...\n",
      "â±ï¸ This may take a while...\n",
      "ğŸ”„ Starting embedding generation...\n",
      "ğŸ“Š Window size: 1\n",
      "ğŸ“Š Patch size: 1\n",
      "ğŸ“Š Batch size: 128\n",
      "ğŸ“Š Device: None\n",
      "âœ… Model set to evaluation mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]\r",
      "1it [00:00,  1.18it/s]\r",
      "2it [00:01,  1.40it/s]\r",
      "3it [00:01,  1.65it/s]\r",
      "4it [00:02,  1.70it/s]\r",
      "5it [00:02,  1.84it/s]\r",
      "6it [00:03,  1.94it/s]\r",
      "7it [00:03,  2.02it/s]\r",
      "8it [00:04,  2.07it/s]\r",
      "9it [00:04,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [00:05,  2.05it/s]\r",
      "11it [00:05,  1.92it/s]\r",
      "12it [00:06,  1.92it/s]\r",
      "13it [00:06,  1.96it/s]\r",
      "14it [00:07,  2.02it/s]\r",
      "15it [00:07,  2.06it/s]\r",
      "16it [00:08,  2.09it/s]\r",
      "17it [00:08,  2.09it/s]\r",
      "18it [00:09,  2.12it/s]\r",
      "19it [00:09,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [00:10,  2.14it/s]\r",
      "21it [00:10,  2.15it/s]\r",
      "22it [00:11,  2.15it/s]\r",
      "23it [00:11,  2.15it/s]\r",
      "24it [00:12,  2.17it/s]\r",
      "25it [00:12,  2.17it/s]\r",
      "26it [00:12,  2.18it/s]\r",
      "27it [00:13,  2.12it/s]\r",
      "28it [00:13,  2.13it/s]\r",
      "29it [00:14,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [00:14,  2.13it/s]\r",
      "31it [00:15,  2.12it/s]\r",
      "32it [00:15,  2.13it/s]\r",
      "33it [00:16,  2.13it/s]\r",
      "34it [00:16,  2.13it/s]\r",
      "35it [00:17,  2.12it/s]\r",
      "36it [00:17,  2.12it/s]\r",
      "37it [00:18,  2.12it/s]\r",
      "38it [00:18,  2.13it/s]\r",
      "39it [00:19,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [00:19,  2.12it/s]\r",
      "41it [00:20,  2.13it/s]\r",
      "42it [00:20,  2.13it/s]\r",
      "43it [00:20,  2.14it/s]\r",
      "44it [00:21,  2.14it/s]\r",
      "45it [00:21,  2.16it/s]\r",
      "46it [00:22,  2.16it/s]\r",
      "47it [00:22,  2.12it/s]\r",
      "48it [00:23,  2.10it/s]\r",
      "49it [00:23,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "50it [00:24,  2.10it/s]\r",
      "51it [00:24,  2.11it/s]\r",
      "52it [00:25,  2.13it/s]\r",
      "53it [00:25,  2.11it/s]\r",
      "54it [00:26,  2.12it/s]\r",
      "55it [00:26,  2.11it/s]\r",
      "56it [00:27,  2.12it/s]\r",
      "57it [00:27,  2.12it/s]\r",
      "58it [00:28,  2.13it/s]\r",
      "59it [00:28,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "60it [00:28,  2.12it/s]\r",
      "61it [00:29,  2.13it/s]\r",
      "62it [00:29,  2.13it/s]\r",
      "63it [00:30,  2.14it/s]\r",
      "64it [00:30,  2.13it/s]\r",
      "65it [00:31,  2.14it/s]\r",
      "66it [00:31,  2.14it/s]\r",
      "67it [00:32,  1.99it/s]\r",
      "68it [00:32,  2.03it/s]\r",
      "69it [00:33,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 70...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "70it [00:33,  2.09it/s]\r",
      "71it [00:34,  2.13it/s]\r",
      "72it [00:34,  2.14it/s]\r",
      "73it [00:35,  2.16it/s]\r",
      "74it [00:35,  2.16it/s]\r",
      "75it [00:36,  2.10it/s]\r",
      "76it [00:36,  2.02it/s]\r",
      "77it [00:37,  2.02it/s]\r",
      "78it [00:37,  2.06it/s]\r",
      "79it [00:38,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 80...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "80it [00:38,  2.09it/s]\r",
      "81it [00:38,  2.12it/s]\r",
      "82it [00:39,  2.14it/s]\r",
      "83it [00:39,  2.12it/s]\r",
      "84it [00:40,  2.14it/s]\r",
      "85it [00:40,  2.15it/s]\r",
      "86it [00:41,  2.17it/s]\r",
      "87it [00:41,  2.17it/s]\r",
      "88it [00:42,  2.17it/s]\r",
      "89it [00:42,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing batch 90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "90it [00:43,  2.18it/s]\r",
      "91it [00:43,  2.26it/s]\r",
      "91it [00:43,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 91 batches total\n",
      "ğŸ”„ Concatenating embeddings...\n",
      "ğŸ“Š Concatenated embeddings shape: (11628, 128)\n",
      "ğŸ“Š Reshaping: height_batches=102, width_batches=114\n",
      "ğŸ“Š Final embeddings shape: (102, 114, 128)\n",
      "âœ… Embeddings generated! Shape: (102, 114, 128)\n",
      "ğŸ”„ Flattening embeddings for clustering...\n",
      "ğŸ“Š Flattened embeddings shape: (11628, 128)\n",
      "ğŸ“Š Embedding dimension: 128\n",
      "ğŸ“Š Number of pixels: 11628\n",
      "âœ… Embeddings ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ STEP 5: Generating embeddings...\")\n",
    "print(\"â±ï¸ This may take a while...\")\n",
    "\n",
    "embeddings = make_embeddings(model, dataset_output, 1, 1, 128)\n",
    "print(f\"âœ… Embeddings generated! Shape: {embeddings.shape}\")\n",
    "\n",
    "print(\"ğŸ”„ Flattening embeddings for clustering...\")\n",
    "embeddings_flat = rearrange(embeddings, \"h w d -> (h w) d\")\n",
    "print(f\"ğŸ“Š Flattened embeddings shape: {embeddings_flat.shape}\")\n",
    "print(f\"ğŸ“Š Embedding dimension: {embeddings_flat.shape[1]}\")\n",
    "print(f\"ğŸ“Š Number of pixels: {embeddings_flat.shape[0]}\")\n",
    "print(\"âœ… Embeddings ready for analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "name": "cluster_embeddings"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ STEP 6: Performing K-means clustering...\n",
      "ğŸ“Š Using 3 clusters\n",
      "ğŸ”„ Fitting K-means model...\n",
      "âœ… K-means clustering complete!\n",
      "ğŸ“Š Labels shape: (11628,)\n",
      "ğŸ“Š Unique labels: [0 1 2]\n",
      "ğŸ“Š Label counts: [4122 7024  482]\n",
      "ğŸ”„ Reshaping labels to image format...\n",
      "ğŸ“Š Reshaped labels shape: (102, 114)\n",
      "âœ… K-means clustering analysis complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ STEP 6: Performing K-means clustering...\")\n",
    "print(\"ğŸ“Š Using 3 clusters\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "print(\"ğŸ”„ Fitting K-means model...\")\n",
    "labels = kmeans.fit_predict(embeddings_flat)\n",
    "print(\"âœ… K-means clustering complete!\")\n",
    "print(f\"ğŸ“Š Labels shape: {labels.shape}\")\n",
    "print(f\"ğŸ“Š Unique labels: {np.unique(labels)}\")\n",
    "print(f\"ğŸ“Š Label counts: {np.bincount(labels)}\")\n",
    "\n",
    "print(\"ğŸ”„ Reshaping labels to image format...\")\n",
    "labels = rearrange(labels, \"(h w) -> h w\", h=embeddings.shape[0], w=embeddings.shape[1])\n",
    "print(f\"ğŸ“Š Reshaped labels shape: {labels.shape}\")\n",
    "print(\"âœ… K-means clustering analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "name": "reduce_dimensions"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ STEP 7: Performing PCA dimensionality reduction...\n",
      "ğŸ“Š Reducing to 3 components for RGB visualization\n",
      "ğŸ”„ Fitting PCA model...\n",
      "âœ… PCA complete!\n",
      "ğŸ“Š PCA embeddings shape: (11628, 3)\n",
      "ğŸ“Š Explained variance ratio: [0.90551436 0.03274911 0.02369779]\n",
      "ğŸ“Š Total explained variance: 0.962\n",
      "ğŸ”„ Reshaping PCA embeddings to image format...\n",
      "ğŸ“Š Reshaped PCA embeddings shape: (102, 114, 3)\n",
      "âœ… PCA dimensionality reduction complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ STEP 7: Performing PCA dimensionality reduction...\")\n",
    "print(\"ğŸ“Š Reducing to 3 components for RGB visualization\")\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "print(\"ğŸ”„ Fitting PCA model...\")\n",
    "embeddings_pca = pca.fit_transform(embeddings_flat)\n",
    "print(\"âœ… PCA complete!\")\n",
    "print(f\"ğŸ“Š PCA embeddings shape: {embeddings_pca.shape}\")\n",
    "print(f\"ğŸ“Š Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"ğŸ“Š Total explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "print(\"ğŸ”„ Reshaping PCA embeddings to image format...\")\n",
    "embeddings_reduced = rearrange(\n",
    "    embeddings_pca, \"(h w) d -> h w d\", h=embeddings.shape[0], w=embeddings.shape[1]\n",
    ")\n",
    "print(f\"ğŸ“Š Reshaped PCA embeddings shape: {embeddings_reduced.shape}\")\n",
    "print(\"âœ… PCA dimensionality reduction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "name": "plot_results"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ STEP 8: Creating visualizations...\n",
      "ğŸ“Š Visualization 1: K-means clustering results\n",
      "âœ… K-means visualization complete!\n",
      "ğŸ“Š Visualization 2: PCA-reduced embeddings as RGB\n",
      "ğŸ“Š PCA RGB range: [0.000, 1.000]\n",
      "âœ… PCA visualization complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ ALL STEPS COMPLETED SUCCESSFULLY!\n",
      "ğŸ“Š Summary:\n",
      "   - Generated embeddings with shape: (102, 114, 3)\n",
      "   - Performed K-means clustering with 3 clusters\n",
      "   - Reduced dimensionality with PCA\n",
      "   - Created 2 visualizations\n",
      "ğŸ¯ The Marimo notebook has run successfully with detailed logging!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ STEP 8: Creating visualizations...\")\n",
    "\n",
    "# Plot the K-means clustering results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "print(\"ğŸ“Š Visualization 1: K-means clustering results\")\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(labels, cmap=\"tab10\")\n",
    "plt.title(\"K-means Clustering (3 clusters)\")\n",
    "plt.colorbar()\n",
    "print(\"âœ… K-means visualization complete!\")\n",
    "\n",
    "# Plot the PCA-reduced embeddings as RGB\n",
    "print(\"ğŸ“Š Visualization 2: PCA-reduced embeddings as RGB\")\n",
    "plt.subplot(1, 3, 2)\n",
    "# Normalize to 0-1 range for display\n",
    "embeddings_normalized = (embeddings_reduced - embeddings_reduced.min()) / (\n",
    "    embeddings_reduced.max() - embeddings_reduced.min()\n",
    ")\n",
    "print(\n",
    "    f\"ğŸ“Š PCA RGB range: [{embeddings_normalized.min():.3f}, {embeddings_normalized.max():.3f}]\"\n",
    ")\n",
    "\n",
    "plt.imshow(embeddings_normalized)\n",
    "plt.title(\"PCA-reduced embeddings (RGB)\")\n",
    "print(\"âœ… PCA visualization complete!\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ‰ ALL STEPS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"ğŸ“Š Summary:\")\n",
    "print(f\"   - Generated embeddings with shape: {embeddings_reduced.shape}\")\n",
    "print(\"   - Performed K-means clustering with 3 clusters\")\n",
    "print(\"   - Reduced dimensionality with PCA\")\n",
    "print(\"   - Created 2 visualizations\")\n",
    "print(\"ğŸ¯ The Marimo notebook has run successfully with detailed logging!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
